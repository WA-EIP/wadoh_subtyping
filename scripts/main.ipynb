{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e03eee-1895-4db6-a183-8718724eb62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37cae34b-2d3b-4556-93a0-6fb56cc4d1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abafbb34-c29d-48aa-9896-0e28125f7ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from sparkpl.converter import spark_to_polars, polars_to_spark\n",
    "\n",
    "from wadoh_raccoon.utils import helpers\n",
    "from wadoh_subtyping import transform as tf, qa\n",
    "from databricks.sdk.runtime import *\n",
    "\n",
    "# import local functions\n",
    "import processor\n",
    "import write\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b25d64c-34ba-4ac9-ad3f-d01611e5fddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the whole process\n",
    "res = processor.match_phl()\n",
    "# breakpoint()\n",
    "# run the rematch process\n",
    "rematch = processor.rematch(pull_res=res.pull_res)\n",
    "\n",
    "temp = (\n",
    "    rematch.rematch_exact_matched\n",
    "    .select(~cs.ends_with(\"_right\"))\n",
    "    .select(~cs.ends_with(\"_\"))\n",
    "    .select(~cs.ends_with(\"_em\"))\n",
    ")\n",
    "\n",
    "# transform and output final roster\n",
    "# first need to combine with rematch roster\n",
    "if len(temp)>0 and len(res.exact_matched_full) > 0:\n",
    "\n",
    "    combined = (pl.concat([res.exact_matched_full,temp]))\n",
    "\n",
    "    print('rematch records found, append to roster')\n",
    "    create_roster = (\n",
    "        tf.create_roster(\n",
    "            matched_and_transformed_df=combined,\n",
    "            respnet=res.pull_res.respnet\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_roster = (tf.dedup_roster(roster_inp=create_roster,reference_inp=res.pull_res.respnet))\n",
    "\n",
    "elif len(temp)==0 and len(res.exact_matched_full) > 0:\n",
    "    print(\"exact matches found in main, no rematches found\")\n",
    "    create_roster = (\n",
    "        tf.create_roster(\n",
    "            matched_and_transformed_df=res.exact_matched_full,\n",
    "            respnet=res.pull_res.respnet\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_roster = (tf.dedup_roster(roster_inp=create_roster,reference_inp=res.pull_res.respnet))\n",
    "\n",
    "elif len(temp)>0 and len(res.exact_matched_full)==0:\n",
    "    print(\"rematch matches found, no main exact mains though.\")\n",
    "    create_roster = (\n",
    "        tf.create_roster(\n",
    "            matched_and_transformed_df=temp,\n",
    "            respnet=res.pull_res.respnet\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_roster = (tf.dedup_roster(roster_inp=create_roster,reference_inp=res.pull_res.respnet))\n",
    "\n",
    "elif len(temp)==0 and len(res.exact_matched_full)==0:\n",
    "    print('Exit: both rematch and normal roster are empty')\n",
    "    final_roster = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d54106-a9ca-47ce-bde4-32ee791b911b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b079fa9e-d184-4cb6-95c0-e0325c836413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if write_data:\n",
    "    try: \n",
    "        review_cols = [\n",
    "            'submission_number',\n",
    "            'internal_create_date',\n",
    "            'CASE_ID',\n",
    "            'first_name_clean',\n",
    "            'last_name_clean',\n",
    "            'first_name_clean_right',\n",
    "            'last_name_clean_right',\n",
    "            'submitted_dob',\n",
    "            'submitted_collection_date',\n",
    "            'reference_collection_date',\n",
    "            'business_day_count',\n",
    "            'match_ratio',\n",
    "            'reverse_match_ratio',\n",
    "            'matched',\n",
    "            'reverse_matched'\n",
    "        ]\n",
    "        # combine the review dfs if they are both filled\n",
    "        if len(rematch.rematch_fuzzy_matched) > 0 and len(res.fuzzy_matched) == 0:\n",
    "            review = (\n",
    "                rematch.rematch_fuzzy_matched\n",
    "                .unique(subset='submission_number')\n",
    "                .select(review_cols)\n",
    "            )\n",
    "        elif len(rematch.rematch_fuzzy_matched) == 0 and len(res.fuzzy_matched) > 0:\n",
    "            review = (\n",
    "                res.fuzzy_matched_full\n",
    "                .unique(subset='submission_number')\n",
    "                .select(review_cols)\n",
    "            )\n",
    "        elif len(rematch.rematch_fuzzy_matched) == 0 and len(res.fuzzy_matched) == 0:\n",
    "            print('\\nNothing to add to review table.')\n",
    "            review = pl.DataFrame()\n",
    "    except ValueError:\n",
    "        print('review dataframe creation didnt work') \n",
    "\n",
    "    if len(review) > 0:\n",
    "\n",
    "        to_write = (\n",
    "            review\n",
    "            .with_columns(\n",
    "                good_match=False,\n",
    "                resolved=False,\n",
    "                notes=None\n",
    "            )\n",
    "            # .write_csv(f\"{dir_name}/fuzzy_matched_review_{date.today()}.csv\")\n",
    "        )\n",
    "\n",
    "        # utils.polars_to_spark_temp_view(spark,to_write,'to_write_df')\n",
    "\n",
    "        spark_df = polars_to_spark(to_write)\n",
    "        spark_df.createOrReplaceTempView(\"to_write_df\")\n",
    "        utils.safe_insert_sql_uc(\n",
    "            spark=spark,\n",
    "            catalog='tc_aim_prod',\n",
    "            schema='diqa_sandbox',\n",
    "            table='fuzzy_review_resp',\n",
    "            source_view='to_write_df',\n",
    "            join_key='submission_number'\n",
    "        )\n",
    "\n",
    "\n",
    "    if isinstance(final_roster, pl.DataFrame) and len(final_roster) > 0:\n",
    "        print('\\nWriting roster to roster table') \n",
    "\n",
    "        # utils.polars_to_spark_temp_view(spark,final_roster,'final_roster_df')\n",
    "        spark_df = polars_to_spark(final_roster)\n",
    "        spark_df.createOrReplaceTempView(\"final_roster_df\")\n",
    "        utils.safe_insert_sql_uc(\n",
    "            spark=spark,\n",
    "            catalog='tc_aim_prod',\n",
    "            schema='diqa_sandbox',\n",
    "            table='roster_resp',\n",
    "            source_view='final_roster_df',\n",
    "            join_key='submission_number'\n",
    "        )\n",
    "    \n",
    "    print('\\nWriting outputs to internal tables')\n",
    "    write.write_phl(pull_res=res.pull_res, w_res=res.w_res, main_res=res)\n",
    "\n",
    "    print('\\nWriting rematch outputs to tables')\n",
    "    write.write_rematch(rematch_res=rematch,pull_res=res.pull_res)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
